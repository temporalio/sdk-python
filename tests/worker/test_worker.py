from __future__ import annotations

import asyncio
import concurrent.futures
import sys
import uuid
from datetime import timedelta
from typing import Any, Awaitable, Callable, Optional, Sequence

import pytest

import temporalio.api.enums.v1
import temporalio.worker._worker
from temporalio import activity, workflow
from temporalio.api.workflowservice.v1 import (
    DescribeWorkerDeploymentRequest,
    DescribeWorkerDeploymentResponse,
    SetWorkerDeploymentCurrentVersionRequest,
    SetWorkerDeploymentCurrentVersionResponse,
    SetWorkerDeploymentRampingVersionRequest,
    SetWorkerDeploymentRampingVersionResponse,
)
from temporalio.client import BuildIdOpAddNewDefault, Client, TaskReachabilityType
from temporalio.common import RawValue, VersioningBehavior
from temporalio.service import RPCError
from temporalio.testing import WorkflowEnvironment
from temporalio.worker import (
    ActivitySlotInfo,
    CustomSlotSupplier,
    FixedSizeSlotSupplier,
    LocalActivitySlotInfo,
    ResourceBasedSlotConfig,
    ResourceBasedSlotSupplier,
    ResourceBasedTunerConfig,
    SlotMarkUsedContext,
    SlotPermit,
    SlotReleaseContext,
    SlotReserveContext,
    Worker,
    WorkerDeploymentConfig,
    WorkerDeploymentVersion,
    WorkerTuner,
    WorkflowSlotInfo,
)
from temporalio.workflow import VersioningIntent
from tests.helpers import assert_eventually, new_worker, worker_versioning_enabled


def test_load_default_worker_binary_id():
    # Just run it twice and confirm it didn't change
    val1 = temporalio.worker._worker.load_default_build_id(memoize=False)
    val2 = temporalio.worker._worker.load_default_build_id(memoize=False)
    assert val1 == val2


@activity.defn
async def never_run_activity() -> None:
    raise NotImplementedError


@workflow.defn
class NeverRunWorkflow:
    @workflow.run
    async def run(self) -> None:
        raise NotImplementedError


async def test_worker_fatal_error_run(client: Client):
    # Run worker with injected workflow poll error
    worker = create_worker(client)
    with pytest.raises(RuntimeError) as err:
        with WorkerFailureInjector(worker) as inj:
            inj.workflow.poll_fail_queue.put_nowait(RuntimeError("OH NO"))
            await worker.run()
    assert str(err.value) == "Workflow worker failed"
    assert err.value.__cause__ and str(err.value.__cause__) == "OH NO"

    # Run worker with injected activity poll error
    worker = create_worker(client)
    with pytest.raises(RuntimeError) as err:
        with WorkerFailureInjector(worker) as inj:
            inj.activity.poll_fail_queue.put_nowait(RuntimeError("OH NO"))
            await worker.run()
    assert str(err.value) == "Activity worker failed"
    assert err.value.__cause__ and str(err.value.__cause__) == "OH NO"

    # Run worker with them both injected (was causing warning for not retrieving
    # the second error)
    worker = create_worker(client)
    with pytest.raises(RuntimeError) as err:
        with WorkerFailureInjector(worker) as inj:
            inj.workflow.poll_fail_queue.put_nowait(RuntimeError("OH NO"))
            inj.activity.poll_fail_queue.put_nowait(RuntimeError("OH NO"))
            await worker.run()
    assert str(err.value).endswith("worker failed")
    assert err.value.__cause__ and str(err.value.__cause__) == "OH NO"


async def test_worker_fatal_error_with(client: Client):
    # Start the worker, wait a short bit, fail it, wait for long time (will be
    # cancelled)
    worker = create_worker(client)
    with pytest.raises(RuntimeError) as err:
        with WorkerFailureInjector(worker) as inj:
            async with worker:
                await asyncio.sleep(0.1)
                inj.workflow.poll_fail_queue.put_nowait(RuntimeError("OH NO"))
                await asyncio.sleep(1000)
    assert str(err.value) == "Workflow worker failed"
    assert err.value.__cause__ and str(err.value.__cause__) == "OH NO"

    # Raise inside the async with and confirm it works
    worker = create_worker(client)
    with pytest.raises(RuntimeError) as err:
        with WorkerFailureInjector(worker) as inj:
            async with worker:
                raise RuntimeError("IN WITH")
    assert str(err.value) == "IN WITH"

    # Demonstrate that inner re-thrown failure swallows worker failure
    worker = create_worker(client)
    with pytest.raises(RuntimeError) as err:
        with WorkerFailureInjector(worker) as inj:
            async with worker:
                inj.workflow.poll_fail_queue.put_nowait(RuntimeError("OH NO"))
                try:
                    await asyncio.sleep(1000)
                except BaseException as inner_err:
                    raise RuntimeError("Caught cancel") from inner_err
    assert str(err.value) == "Caught cancel"
    assert err.value.__cause__ and type(err.value.__cause__) is asyncio.CancelledError


async def test_worker_fatal_error_callback(client: Client):
    callback_err: Optional[BaseException] = None

    async def on_fatal_error(exc: BaseException) -> None:
        nonlocal callback_err
        callback_err = exc

    worker = create_worker(client, on_fatal_error)
    with pytest.raises(RuntimeError) as err:
        with WorkerFailureInjector(worker) as inj:
            async with worker:
                await asyncio.sleep(0.1)
                inj.workflow.poll_fail_queue.put_nowait(RuntimeError("OH NO"))
                await asyncio.sleep(1000)
    assert err.value is callback_err


async def test_worker_cancel_run(client: Client):
    worker = create_worker(client)
    assert not worker.is_running and not worker.is_shutdown
    run_task = asyncio.create_task(worker.run())
    await asyncio.sleep(0.3)
    assert worker.is_running and not worker.is_shutdown
    run_task.cancel()
    with pytest.raises(asyncio.CancelledError):
        await run_task
    assert not worker.is_running and worker.is_shutdown


@activity.defn
async def say_hello(name: str) -> str:
    return f"Hello, {name}!"


@workflow.defn
class WaitOnSignalWorkflow:
    def __init__(self) -> None:
        self._last_signal = "<none>"

    @workflow.run
    async def run(self) -> None:
        await workflow.wait_condition(lambda: self._last_signal == "finish")
        await workflow.execute_activity(
            say_hello,
            "hi",
            versioning_intent=VersioningIntent.DEFAULT,
            start_to_close_timeout=timedelta(seconds=5),
        )

    @workflow.signal
    def my_signal(self, value: str) -> None:
        self._last_signal = value
        workflow.logger.info(f"Signal: {value}")


async def test_worker_versioning(client: Client, env: WorkflowEnvironment):
    if env.supports_time_skipping:
        pytest.skip("Java test server does not support worker versioning")
    if not await worker_versioning_enabled(client):
        pytest.skip("This server does not have worker versioning enabled")

    task_queue = f"worker-versioning-{uuid.uuid4()}"
    await client.update_worker_build_id_compatibility(
        task_queue, BuildIdOpAddNewDefault("1.0")
    )

    async with new_worker(
        client,
        WaitOnSignalWorkflow,
        activities=[say_hello],
        task_queue=task_queue,
        build_id="1.0",
        use_worker_versioning=True,
    ):
        wf1 = await client.start_workflow(
            WaitOnSignalWorkflow.run,
            id=f"worker-versioning-1-{uuid.uuid4()}",
            task_queue=task_queue,
        )
        # Sleep for a beat, otherwise it's possible for new workflow to start on 2.0
        await asyncio.sleep(0.1)
        await client.update_worker_build_id_compatibility(
            task_queue, BuildIdOpAddNewDefault("2.0")
        )
        wf2 = await client.start_workflow(
            WaitOnSignalWorkflow.run,
            id=f"worker-versioning-2-{uuid.uuid4()}",
            task_queue=task_queue,
        )
        async with new_worker(
            client,
            WaitOnSignalWorkflow,
            activities=[say_hello],
            task_queue=task_queue,
            build_id="2.0",
            use_worker_versioning=True,
        ):
            # Confirm reachability type parameter is respected. If it wasn't, list would have
            # `OPEN_WORKFLOWS` in it.
            reachability = await client.get_worker_task_reachability(
                build_ids=["2.0"],
                reachability_type=TaskReachabilityType.CLOSED_WORKFLOWS,
            )
            assert reachability.build_id_reachability["2.0"].task_queue_reachability[
                task_queue
            ] == [TaskReachabilityType.NEW_WORKFLOWS]

            await wf1.signal(WaitOnSignalWorkflow.my_signal, "finish")
            await wf2.signal(WaitOnSignalWorkflow.my_signal, "finish")
            await wf1.result()
            await wf2.result()


async def test_worker_validate_fail(client: Client, env: WorkflowEnvironment):
    if env.supports_time_skipping:
        pytest.skip("Java test server does not appear to fail on invalid namespace")
    # Try to run a worker on an invalid namespace
    config = client.config()
    config["namespace"] = "does-not-exist"
    client = Client(**config)
    with pytest.raises(RuntimeError) as err:
        await Worker(
            client, task_queue=f"tq-{uuid.uuid4()}", workflows=[NeverRunWorkflow]
        ).run()
    assert str(err.value).startswith("Worker validation failed")


async def test_can_run_resource_based_worker(client: Client, env: WorkflowEnvironment):
    tuner = WorkerTuner.create_resource_based(
        target_memory_usage=0.5,
        target_cpu_usage=0.5,
        workflow_config=ResourceBasedSlotConfig(5, 20, timedelta(seconds=0)),
        # Ensure we can assume defaults when specifying only some options
        activity_config=ResourceBasedSlotConfig(minimum_slots=1),
    )
    async with new_worker(
        client,
        WaitOnSignalWorkflow,
        activities=[say_hello],
        tuner=tuner,
    ) as w:
        wf1 = await client.start_workflow(
            WaitOnSignalWorkflow.run,
            id=f"resource-based-{uuid.uuid4()}",
            task_queue=w.task_queue,
        )
        await wf1.signal(WaitOnSignalWorkflow.my_signal, "finish")
        await wf1.result()


async def test_can_run_composite_tuner_worker(client: Client, env: WorkflowEnvironment):
    resource_based_options = ResourceBasedTunerConfig(0.5, 0.5)
    tuner = WorkerTuner.create_composite(
        workflow_supplier=FixedSizeSlotSupplier(5),
        activity_supplier=ResourceBasedSlotSupplier(
            ResourceBasedSlotConfig(
                minimum_slots=1,
                maximum_slots=20,
                ramp_throttle=timedelta(milliseconds=60),
            ),
            resource_based_options,
        ),
        local_activity_supplier=ResourceBasedSlotSupplier(
            ResourceBasedSlotConfig(
                minimum_slots=1,
                maximum_slots=5,
                ramp_throttle=timedelta(milliseconds=60),
            ),
            resource_based_options,
        ),
    )
    async with new_worker(
        client,
        WaitOnSignalWorkflow,
        activities=[say_hello],
        tuner=tuner,
    ) as w:
        wf1 = await client.start_workflow(
            WaitOnSignalWorkflow.run,
            id=f"composite-tuner-{uuid.uuid4()}",
            task_queue=w.task_queue,
        )
        await wf1.signal(WaitOnSignalWorkflow.my_signal, "finish")
        await wf1.result()


async def test_cant_specify_max_concurrent_and_tuner(
    client: Client, env: WorkflowEnvironment
):
    tuner = WorkerTuner.create_resource_based(
        target_memory_usage=0.5,
        target_cpu_usage=0.5,
        workflow_config=ResourceBasedSlotConfig(5, 20, timedelta(seconds=0)),
    )
    with pytest.raises(ValueError) as err:
        async with new_worker(
            client,
            WaitOnSignalWorkflow,
            activities=[say_hello],
            tuner=tuner,
            max_concurrent_workflow_tasks=10,
        ):
            pass
    assert "Cannot specify " in str(err.value)
    assert "when also specifying tuner" in str(err.value)


async def test_warns_when_workers_too_lot(client: Client, env: WorkflowEnvironment):
    tuner = WorkerTuner.create_resource_based(
        target_memory_usage=0.5,
        target_cpu_usage=0.5,
    )
    with concurrent.futures.ThreadPoolExecutor() as executor:
        with pytest.warns(
            UserWarning,
            match="Worker max_concurrent_activities is 500 but activity_executor's max_workers is only",
        ):
            async with new_worker(
                client,
                WaitOnSignalWorkflow,
                activities=[say_hello],
                tuner=tuner,
                activity_executor=executor,
            ):
                pass


async def test_custom_slot_supplier(client: Client, env: WorkflowEnvironment):
    class MyPermit(SlotPermit):
        def __init__(self, pnum: int):
            super().__init__()
            self.pnum = pnum

    class MySlotSupplier(CustomSlotSupplier):
        reserves = 0
        releases = 0
        highest_seen_reserve_on_release = 0
        used = 0
        seen_sticky_kinds = set()
        seen_slot_kinds = set()
        seen_used_slot_kinds = set()
        seen_release_info_empty = False
        seen_release_info_nonempty = False

        async def reserve_slot(self, ctx: SlotReserveContext) -> SlotPermit:
            self.reserve_asserts(ctx)
            # Verify an async call doesn't bungle things
            await asyncio.sleep(0.01)
            self.reserves += 1
            return MyPermit(self.reserves)

        def try_reserve_slot(self, ctx: SlotReserveContext) -> Optional[SlotPermit]:
            self.reserve_asserts(ctx)
            return None

        def mark_slot_used(self, ctx: SlotMarkUsedContext) -> None:
            assert ctx.permit is not None
            assert isinstance(ctx.permit, MyPermit)
            assert ctx.permit.pnum is not None
            assert ctx.slot_info is not None
            if isinstance(ctx.slot_info, WorkflowSlotInfo):
                self.seen_used_slot_kinds.add("wf")
            elif isinstance(ctx.slot_info, ActivitySlotInfo):
                self.seen_used_slot_kinds.add("a")
            elif isinstance(ctx.slot_info, LocalActivitySlotInfo):
                self.seen_used_slot_kinds.add("la")
            self.used += 1

        def release_slot(self, ctx: SlotReleaseContext) -> None:
            assert ctx.permit is not None
            assert isinstance(ctx.permit, MyPermit)
            assert ctx.permit.pnum is not None
            self.highest_seen_reserve_on_release = max(
                ctx.permit.pnum, self.highest_seen_reserve_on_release
            )
            # Info may be empty, and we should see both empty and not
            if ctx.slot_info is None:
                self.seen_release_info_empty = True
            else:
                self.seen_release_info_nonempty = True
            self.releases += 1

        def reserve_asserts(self, ctx):
            assert ctx.task_queue is not None
            assert ctx.worker_identity is not None
            assert ctx.worker_build_id is not None
            self.seen_sticky_kinds.add(ctx.is_sticky)
            self.seen_slot_kinds.add(ctx.slot_type)

    ss = MySlotSupplier()

    tuner = WorkerTuner.create_composite(
        workflow_supplier=ss, activity_supplier=ss, local_activity_supplier=ss
    )
    async with new_worker(
        client,
        WaitOnSignalWorkflow,
        activities=[say_hello],
        tuner=tuner,
        identity="myworker",
    ) as w:
        wf1 = await client.start_workflow(
            WaitOnSignalWorkflow.run,
            id=f"custom-slot-supplier-{uuid.uuid4()}",
            task_queue=w.task_queue,
        )
        await wf1.signal(WaitOnSignalWorkflow.my_signal, "finish")
        await wf1.result()

    # We can't use reserve number directly because there is a technically possible race
    # where the python reserve function appears to complete, but Rust doesn't see that.
    # This isn't solvable without redoing a chunk of pyo3-asyncio. So we only check
    # that the permits passed to release line up.
    assert ss.highest_seen_reserve_on_release == ss.releases
    # Two workflow tasks, one activity
    assert ss.used == 3
    assert ss.seen_sticky_kinds == {True, False}
    assert ss.seen_slot_kinds == {"workflow", "activity", "local-activity"}
    assert ss.seen_used_slot_kinds == {"wf", "a"}
    assert ss.seen_release_info_empty
    assert ss.seen_release_info_nonempty


@workflow.defn
class SimpleWorkflow:
    @workflow.run
    async def run(self) -> str:
        return "hi"


async def test_throwing_slot_supplier(client: Client, env: WorkflowEnvironment):
    """Ensures a (mostly) broken slot supplier doesn't hose everything up"""

    class ThrowingSlotSupplier(CustomSlotSupplier):
        marked_used = False

        async def reserve_slot(self, ctx: SlotReserveContext) -> SlotPermit:
            # Hand out workflow tasks until one is used
            if ctx.slot_type == "workflow" and not self.marked_used:
                return SlotPermit()
            raise ValueError("I always throw")

        def try_reserve_slot(self, ctx: SlotReserveContext) -> Optional[SlotPermit]:
            raise ValueError("I always throw")

        def mark_slot_used(self, ctx: SlotMarkUsedContext) -> None:
            raise ValueError("I always throw")

        def release_slot(self, ctx: SlotReleaseContext) -> None:
            raise ValueError("I always throw")

    ss = ThrowingSlotSupplier()

    tuner = WorkerTuner.create_composite(
        workflow_supplier=ss, activity_supplier=ss, local_activity_supplier=ss
    )
    async with new_worker(
        client,
        SimpleWorkflow,
        activities=[say_hello],
        tuner=tuner,
    ) as w:
        wf1 = await client.start_workflow(
            SimpleWorkflow.run,
            id=f"throwing-slot-supplier-{uuid.uuid4()}",
            task_queue=w.task_queue,
        )
        await wf1.result()


async def test_blocking_slot_supplier(client: Client, env: WorkflowEnvironment):
    class BlockingSlotSupplier(CustomSlotSupplier):
        marked_used = False

        async def reserve_slot(self, ctx: SlotReserveContext) -> SlotPermit:
            await asyncio.get_event_loop().create_future()
            raise ValueError("Should be unreachable")

        def try_reserve_slot(self, ctx: SlotReserveContext) -> Optional[SlotPermit]:
            return None

        def mark_slot_used(self, ctx: SlotMarkUsedContext) -> None:
            return None

        def release_slot(self, ctx: SlotReleaseContext) -> None:
            return None

    ss = BlockingSlotSupplier()

    tuner = WorkerTuner.create_composite(
        workflow_supplier=ss, activity_supplier=ss, local_activity_supplier=ss
    )
    async with new_worker(
        client,
        SimpleWorkflow,
        activities=[say_hello],
        tuner=tuner,
    ) as _w:
        await asyncio.sleep(1)


@workflow.defn(
    name="DeploymentVersioningWorkflow",
    versioning_behavior=VersioningBehavior.AUTO_UPGRADE,
)
class DeploymentVersioningWorkflowV1AutoUpgrade:
    def __init__(self) -> None:
        self.finish = False

    @workflow.run
    async def run(self):
        await workflow.wait_condition(lambda: self.finish)
        return "version-v1"

    @workflow.signal
    def do_finish(self):
        self.finish = True

    @workflow.query
    def state(self):
        return "v1"


@workflow.defn(
    name="DeploymentVersioningWorkflow", versioning_behavior=VersioningBehavior.PINNED
)
class DeploymentVersioningWorkflowV2Pinned:
    def __init__(self) -> None:
        self.finish = False

    @workflow.run
    async def run(self):
        await workflow.wait_condition(lambda: self.finish)
        depver = workflow.info().get_current_deployment_version()
        # assert depver
        # assert depver.build_id == "2.0"
        # Just ensuring the rust object was converted properly and this method still works
        # workflow.logger.debug(f"Dep string: {depver.to_canonical_string()}")
        return "version-v2"

    @workflow.signal
    def do_finish(self):
        self.finish = True

    @workflow.query
    def state(self):
        return "v2"


@workflow.defn(
    name="DeploymentVersioningWorkflow",
    versioning_behavior=VersioningBehavior.AUTO_UPGRADE,
)
class DeploymentVersioningWorkflowV3AutoUpgrade:
    def __init__(self) -> None:
        self.finish = False

    @workflow.run
    async def run(self):
        await workflow.wait_condition(lambda: self.finish)
        return "version-v3"

    @workflow.signal
    def do_finish(self):
        self.finish = True

    @workflow.query
    def state(self):
        return "v3"


@pytest.mark.skipif(
    sys.version_info < (3, 12), reason="Skipping for < 3.12 due to import race bug"
)
async def test_worker_with_worker_deployment_config(
    client: Client, env: WorkflowEnvironment
):
    if env.supports_time_skipping:
        pytest.skip("Test Server doesn't support worker deployments")

    deployment_name = f"deployment-{uuid.uuid4()}"
    worker_v1 = WorkerDeploymentVersion(deployment_name=deployment_name, build_id="1.0")
    worker_v2 = WorkerDeploymentVersion(deployment_name=deployment_name, build_id="2.0")
    worker_v3 = WorkerDeploymentVersion(deployment_name=deployment_name, build_id="3.0")
    async with (
        new_worker(
            client,
            DeploymentVersioningWorkflowV1AutoUpgrade,
            deployment_config=WorkerDeploymentConfig(
                version=worker_v1,
                use_worker_versioning=True,
            ),
        ) as w1,
        new_worker(
            client,
            DeploymentVersioningWorkflowV2Pinned,
            deployment_config=WorkerDeploymentConfig(
                version=worker_v2,
                use_worker_versioning=True,
            ),
            task_queue=w1.task_queue,
        ),
        new_worker(
            client,
            DeploymentVersioningWorkflowV3AutoUpgrade,
            deployment_config=WorkerDeploymentConfig(
                version=worker_v3,
                use_worker_versioning=True,
            ),
            task_queue=w1.task_queue,
        ),
    ):
        describe_resp = await wait_until_worker_deployment_visible(
            client,
            worker_v1,
        )
        await set_current_deployment_version(
            client, describe_resp.conflict_token, worker_v1
        )

        # Start workflow 1 which will use the 1.0 worker on auto-upgrade
        wf1 = await client.start_workflow(
            DeploymentVersioningWorkflowV1AutoUpgrade.run,
            id="basic-versioning-v1",
            task_queue=w1.task_queue,
        )
        assert "v1" == await wf1.query("state")

        describe_resp2 = await wait_until_worker_deployment_visible(client, worker_v2)
        await set_current_deployment_version(
            client, describe_resp2.conflict_token, worker_v2
        )

        wf2 = await client.start_workflow(
            DeploymentVersioningWorkflowV2Pinned.run,
            id="basic-versioning-v2",
            task_queue=w1.task_queue,
        )
        assert "v2" == await wf2.query("state")

        describe_resp3 = await wait_until_worker_deployment_visible(client, worker_v3)
        await set_current_deployment_version(
            client, describe_resp3.conflict_token, worker_v3
        )

        wf3 = await client.start_workflow(
            DeploymentVersioningWorkflowV3AutoUpgrade.run,
            id="basic-versioning-v3",
            task_queue=w1.task_queue,
        )
        assert "v3" == await wf3.query("state")

        # Signal all workflows to finish
        await wf1.signal(DeploymentVersioningWorkflowV1AutoUpgrade.do_finish)
        await wf2.signal(DeploymentVersioningWorkflowV2Pinned.do_finish)
        await wf3.signal(DeploymentVersioningWorkflowV3AutoUpgrade.do_finish)

        res1 = await wf1.result()
        res2 = await wf2.result()
        res3 = await wf3.result()

        assert res1 == "version-v3"
        assert res2 == "version-v2"
        assert res3 == "version-v3"


@pytest.mark.skipif(
    sys.version_info < (3, 12), reason="Skipping for < 3.12 due to import race bug"
)
async def test_worker_deployment_ramp(client: Client, env: WorkflowEnvironment):
    if env.supports_time_skipping:
        pytest.skip("Test Server doesn't support worker deployments")

    deployment_name = f"deployment-ramping-{uuid.uuid4()}"
    v1 = WorkerDeploymentVersion(deployment_name=deployment_name, build_id="1.0")
    v2 = WorkerDeploymentVersion(deployment_name=deployment_name, build_id="2.0")
    async with (
        new_worker(
            client,
            DeploymentVersioningWorkflowV1AutoUpgrade,
            deployment_config=WorkerDeploymentConfig(
                version=v1, use_worker_versioning=True
            ),
        ) as w1,
        new_worker(
            client,
            DeploymentVersioningWorkflowV2Pinned,
            deployment_config=WorkerDeploymentConfig(
                version=v2, use_worker_versioning=True
            ),
            task_queue=w1.task_queue,
        ),
    ):
        await wait_until_worker_deployment_visible(client, v1)
        describe_resp = await wait_until_worker_deployment_visible(client, v2)

        # Set current version to v1 and ramp v2 to 100%
        conflict_token = (
            await set_current_deployment_version(
                client, describe_resp.conflict_token, v1
            )
        ).conflict_token
        conflict_token = (
            await set_ramping_version(client, conflict_token, v2, 100)
        ).conflict_token

        # Run workflows and verify they run on v2
        for i in range(3):
            wf = await client.start_workflow(
                DeploymentVersioningWorkflowV2Pinned.run,
                id=f"versioning-ramp-100-{i}-{uuid.uuid4()}",
                task_queue=w1.task_queue,
            )
            await wf.signal(DeploymentVersioningWorkflowV2Pinned.do_finish)
            res = await wf.result()
            assert res == "version-v2"

        # Set ramp to 0, expecting workflows to run on v1
        conflict_token = (
            await set_ramping_version(client, conflict_token, v2, 0)
        ).conflict_token
        for i in range(3):
            wfa = await client.start_workflow(
                DeploymentVersioningWorkflowV1AutoUpgrade.run,
                id=f"versioning-ramp-0-{i}-{uuid.uuid4()}",
                task_queue=w1.task_queue,
            )
            await wfa.signal(DeploymentVersioningWorkflowV1AutoUpgrade.do_finish)
            res = await wfa.result()
            assert res == "version-v1"

        # Set ramp to 50 and eventually verify workflows run on both versions
        await set_ramping_version(client, conflict_token, v2, 50)
        seen_results = set()

        async def run_and_record():
            wf = await client.start_workflow(
                DeploymentVersioningWorkflowV1AutoUpgrade.run,
                id=f"versioning-ramp-50-{uuid.uuid4()}",
                task_queue=w1.task_queue,
            )
            await wf.signal(DeploymentVersioningWorkflowV1AutoUpgrade.do_finish)
            return await wf.result()

        async def check_results():
            res = await run_and_record()
            seen_results.add(res)
            assert "version-v1" in seen_results and "version-v2" in seen_results

        await assert_eventually(check_results)


@workflow.defn(dynamic=True, versioning_behavior=VersioningBehavior.PINNED)
class DynamicWorkflowVersioningOnDefn:
    @workflow.run
    async def run(self, args: Sequence[RawValue]) -> str:
        return "dynamic"


@pytest.mark.skipif(
    sys.version_info < (3, 12), reason="Skipping for < 3.12 due to import race bug"
)
async def test_worker_deployment_dynamic_workflow_on_run(
    client: Client, env: WorkflowEnvironment
):
    if env.supports_time_skipping:
        pytest.skip("Test Server doesn't support worker deployments")

    deployment_name = f"deployment-dynamic-{uuid.uuid4()}"
    worker_v1 = WorkerDeploymentVersion(deployment_name=deployment_name, build_id="1.0")

    async with new_worker(
        client,
        DynamicWorkflowVersioningOnDefn,
        deployment_config=WorkerDeploymentConfig(
            version=worker_v1,
            use_worker_versioning=True,
        ),
    ) as w:
        describe_resp = await wait_until_worker_deployment_visible(
            client,
            worker_v1,
        )
        await set_current_deployment_version(
            client, describe_resp.conflict_token, worker_v1
        )

        wf = await client.start_workflow(
            "cooldynamicworkflow",
            id=f"dynamic-workflow-versioning-{uuid.uuid4()}",
            task_queue=w.task_queue,
        )
        result = await wf.result()
        assert result == "dynamic"

        history = await wf.fetch_history()
        assert any(
            event.HasField("workflow_task_completed_event_attributes")
            and event.workflow_task_completed_event_attributes.versioning_behavior
            == temporalio.api.enums.v1.VersioningBehavior.VERSIONING_BEHAVIOR_PINNED
            for event in history.events
        )


@workflow.defn
class NoVersioningAnnotationWorkflow:
    @workflow.run
    async def run(self) -> str:
        return "whee"


@workflow.defn(dynamic=True)
class NoVersioningAnnotationDynamicWorkflow:
    @workflow.run
    async def run(self, args: Sequence[RawValue]) -> str:
        return "whee"


@pytest.mark.skipif(
    sys.version_info < (3, 12), reason="Skipping for < 3.12 due to import race bug"
)
async def test_workflows_must_have_versioning_behavior_when_feature_turned_on(
    client: Client, env: WorkflowEnvironment
):
    with pytest.raises(ValueError) as exc_info:
        Worker(
            client,
            task_queue=f"task-queue-{uuid.uuid4()}",
            workflows=[NoVersioningAnnotationWorkflow],
            deployment_config=WorkerDeploymentConfig(
                version=WorkerDeploymentVersion(
                    deployment_name="whatever", build_id="1.0"
                ),
                use_worker_versioning=True,
            ),
        )

    assert "must specify a versioning behavior" in str(exc_info.value)

    with pytest.raises(ValueError) as exc_info:
        Worker(
            client,
            task_queue=f"task-queue-{uuid.uuid4()}",
            workflows=[NoVersioningAnnotationDynamicWorkflow],
            deployment_config=WorkerDeploymentConfig(
                version=WorkerDeploymentVersion(
                    deployment_name="whatever", build_id="1.0"
                ),
                use_worker_versioning=True,
            ),
        )

    assert "must specify a versioning behavior" in str(exc_info.value)


@pytest.mark.skipif(
    sys.version_info < (3, 12), reason="Skipping for < 3.12 due to import race bug"
)
async def test_workflows_can_use_default_versioning_behavior(
    client: Client, env: WorkflowEnvironment
):
    if env.supports_time_skipping:
        pytest.skip("Test Server doesn't support worker versioning")

    deployment_name = f"deployment-default-versioning-{uuid.uuid4()}"
    worker_v1 = WorkerDeploymentVersion(deployment_name=deployment_name, build_id="1.0")

    async with new_worker(
        client,
        NoVersioningAnnotationWorkflow,
        deployment_config=WorkerDeploymentConfig(
            version=worker_v1,
            use_worker_versioning=True,
            default_versioning_behavior=VersioningBehavior.PINNED,
        ),
    ) as w:
        describe_resp = await wait_until_worker_deployment_visible(
            client,
            worker_v1,
        )
        await set_current_deployment_version(
            client, describe_resp.conflict_token, worker_v1
        )

        wf = await client.start_workflow(
            NoVersioningAnnotationWorkflow.run,
            id=f"default-versioning-behavior-{uuid.uuid4()}",
            task_queue=w.task_queue,
        )
        await wf.result()

        history = await wf.fetch_history()
        assert any(
            event.HasField("workflow_task_completed_event_attributes")
            and event.workflow_task_completed_event_attributes.versioning_behavior
            == temporalio.api.enums.v1.VersioningBehavior.VERSIONING_BEHAVIOR_PINNED
            for event in history.events
        )


async def wait_until_worker_deployment_visible(
    client: Client, version: WorkerDeploymentVersion
) -> DescribeWorkerDeploymentResponse:
    async def mk_call() -> DescribeWorkerDeploymentResponse:
        try:
            res = await client.workflow_service.describe_worker_deployment(
                DescribeWorkerDeploymentRequest(
                    namespace=client.namespace,
                    deployment_name=version.deployment_name,
                )
            )
        except RPCError:
            # Expected
            assert False
        assert any(
            vs.version == version.to_canonical_string()
            for vs in res.worker_deployment_info.version_summaries
        )
        return res

    return await assert_eventually(mk_call)


async def set_current_deployment_version(
    client: Client, conflict_token: bytes, version: WorkerDeploymentVersion
) -> SetWorkerDeploymentCurrentVersionResponse:
    return await client.workflow_service.set_worker_deployment_current_version(
        SetWorkerDeploymentCurrentVersionRequest(
            namespace=client.namespace,
            deployment_name=version.deployment_name,
            version=version.to_canonical_string(),
            conflict_token=conflict_token,
        )
    )


async def set_ramping_version(
    client: Client,
    conflict_token: bytes,
    version: WorkerDeploymentVersion,
    percentage: float,
) -> SetWorkerDeploymentRampingVersionResponse:
    response = await client.workflow_service.set_worker_deployment_ramping_version(
        SetWorkerDeploymentRampingVersionRequest(
            namespace=client.namespace,
            deployment_name=version.deployment_name,
            version=version.to_canonical_string(),
            conflict_token=conflict_token,
            percentage=percentage,
        )
    )
    return response


def create_worker(
    client: Client,
    on_fatal_error: Optional[Callable[[BaseException], Awaitable[None]]] = None,
) -> Worker:
    return Worker(
        client,
        task_queue=f"task-queue-{uuid.uuid4()}",
        activities=[never_run_activity],
        workflows=[NeverRunWorkflow],
        on_fatal_error=on_fatal_error,
    )


class WorkerFailureInjector:
    def __init__(self, worker: Worker) -> None:
        self.workflow = PollFailureInjector(worker, "poll_workflow_activation")
        self.activity = PollFailureInjector(worker, "poll_activity_task")

    def __enter__(self) -> WorkerFailureInjector:
        return self

    def __exit__(self, *args, **kwargs) -> None:
        self.workflow.shutdown()
        self.activity.shutdown()


class PollFailureInjector:
    def __init__(self, worker: Worker, attr: str) -> None:
        self.worker = worker
        self.attr = attr
        self.poll_fail_queue: asyncio.Queue[Exception] = asyncio.Queue()
        self.orig_poll_call = getattr(worker._bridge_worker, attr)
        setattr(worker._bridge_worker, attr, self.patched_poll_call)
        self.next_poll_task: Optional[asyncio.Task] = None
        self.next_exception_task: Optional[asyncio.Task] = None

    async def patched_poll_call(self) -> Any:
        if not self.next_poll_task:
            self.next_poll_task = asyncio.create_task(self.orig_poll_call())
        if not self.next_exception_task:
            self.next_exception_task = asyncio.create_task(self.poll_fail_queue.get())

        await asyncio.wait(
            [self.next_poll_task, self.next_exception_task],
            return_when=asyncio.FIRST_COMPLETED,
        )

        # If activation came, return that and leave queue for next poll
        if self.next_poll_task.done():
            ret = self.next_poll_task.result()
            self.next_poll_task = None
            return ret

        # Raise the error
        exc = self.next_exception_task.result()
        self.next_exception_task = None
        raise exc

    def shutdown(self) -> None:
        if self.next_poll_task:
            self.next_poll_task.cancel()
        if self.next_exception_task:
            self.next_exception_task.cancel()
        setattr(self.worker._bridge_worker, self.attr, self.orig_poll_call)
